{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.17.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 05_scores_linear_regression\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.random.set_seed(5)\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 3) (25, 1)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러 오기\n",
    "xy =  np.loadtxt('data-02-test-score.csv',delimiter=',',skiprows=1,\n",
    "                dtype=np.float32)\n",
    "# x_train\n",
    "x_train = xy[:,:-1]  # X\n",
    "y_train = xy[:,[-1]] # Y\n",
    "print(x_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'weight:0' shape=(3, 1) dtype=float32, numpy=\n",
       "array([[-0.18030666],\n",
       "       [-0.95028627],\n",
       "       [-0.03964049]], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 변수 초기화 : weight, bias\n",
    "# Y = X*W + b\n",
    "# (m,n) * (n,l) = (m,l)   : 행렬의 내적 곱셈 공식\n",
    "# (25,3) * (3,3) = (25,1)\n",
    "W = tf.Variable(tf.random.normal([3,1]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 함수(hypothesis) : H(X) = W1*X1 + W2*X2 + W3*X3 + b\n",
    "def hypothesis(X):\n",
    "    return tf.matmul(X,W) + b  # 내적 곱셈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비용 함수 : (Hx - y)^2 의 평균\n",
    "# tf.square()      : 제곱\n",
    "# tf.reduce_mean() : 합의 평균\n",
    "def cost_func():\n",
    "    cost = tf.reduce_mean(tf.square(hypothesis(x_train) - y_train))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\storm\\AppData\\Local\\Temp\\ipykernel_4540\\325047341.py:4: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 경사 하강법\n",
    "# learning_rate(학습율)을 0.01 로 설정하여 optimizer객체를 생성\n",
    "# optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.01)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Start Learning!!\n",
      "0000 cost:[ 65066.566 ]  W: [[-0.17030665]\n",
      " [-0.9402863 ]\n",
      " [-0.02964049]]  b: [0.23652864]\n",
      "0100 cost:[ 4095.1208 ]  W: [[ 0.61485445]\n",
      " [-0.15491953]\n",
      " [ 0.7556197 ]]  b: [1.0216749]\n",
      "0200 cost:[ 47.5905 ]  W: [[0.8524797 ]\n",
      " [0.08370285]\n",
      " [0.993726  ]]  b: [1.2592264]\n",
      "0300 cost:[ 12.429792 ]  W: [[0.8754432]\n",
      " [0.1082766]\n",
      " [1.0174664]]  b: [1.2820698]\n",
      "0400 cost:[ 12.382445 ]  W: [[0.8750678 ]\n",
      " [0.10988382]\n",
      " [1.0180483 ]]  b: [1.2815448]\n",
      "0500 cost:[ 12.352391 ]  W: [[0.87393206]\n",
      " [0.11105198]\n",
      " [1.0180252 ]]  b: [1.2802328]\n",
      "0600 cost:[ 12.318468 ]  W: [[0.8726458 ]\n",
      " [0.11237465]\n",
      " [1.0180013 ]]  b: [1.2787431]\n",
      "0700 cost:[ 12.2807865 ]  W: [[0.87121135]\n",
      " [0.1138463 ]\n",
      " [1.0179775 ]]  b: [1.2770786]\n",
      "0800 cost:[ 12.239475 ]  W: [[0.86963135]\n",
      " [0.11546495]\n",
      " [1.0179536 ]]  b: [1.2752408]\n",
      "0900 cost:[ 12.1945505 ]  W: [[0.8679064]\n",
      " [0.1172296]\n",
      " [1.0179298]]  b: [1.2732286]\n",
      "1000 cost:[ 12.1460905 ]  W: [[0.8660364]\n",
      " [0.1191404]\n",
      " [1.017906 ]]  b: [1.2710408]\n",
      "1100 cost:[ 12.094104 ]  W: [[0.8640209 ]\n",
      " [0.12119825]\n",
      " [1.0178821 ]]  b: [1.2686745]\n",
      "1200 cost:[ 12.038631 ]  W: [[0.8618582 ]\n",
      " [0.12340456]\n",
      " [1.0178583 ]]  b: [1.266126]\n",
      "1300 cost:[ 11.979659 ]  W: [[0.8595456 ]\n",
      " [0.12576105]\n",
      " [1.0178362 ]]  b: [1.2633908]\n",
      "1400 cost:[ 11.917183 ]  W: [[0.8570781 ]\n",
      " [0.12826686]\n",
      " [1.0178217 ]]  b: [1.2604601]\n",
      "1500 cost:[ 11.851211 ]  W: [[0.85445434]\n",
      " [0.13092609]\n",
      " [1.0178097 ]]  b: [1.2573304]\n",
      "1600 cost:[ 11.781746 ]  W: [[0.85167295]\n",
      " [0.13374287]\n",
      " [1.0178012 ]]  b: [1.2539963]\n",
      "1700 cost:[ 11.70879 ]  W: [[0.84872663]\n",
      " [0.1367163 ]\n",
      " [1.0178009 ]]  b: [1.2504464]\n",
      "1800 cost:[ 11.63232 ]  W: [[0.8456147 ]\n",
      " [0.13985221]\n",
      " [1.017807  ]]  b: [1.2466766]\n",
      "1900 cost:[ 11.552404 ]  W: [[0.8423318 ]\n",
      " [0.14315222]\n",
      " [1.0178206 ]]  b: [1.2426767]\n",
      "2000 cost:[ 11.468983 ]  W: [[0.8388735]\n",
      " [0.1466188]\n",
      " [1.0178446]]  b: [1.2384369]\n",
      "2100 cost:[ 11.382096 ]  W: [[0.83523595]\n",
      " [0.15025543]\n",
      " [1.0178801 ]]  b: [1.2339485]\n",
      "2200 cost:[ 11.291774 ]  W: [[0.83141494]\n",
      " [0.15406494]\n",
      " [1.0179286 ]]  b: [1.2292007]\n",
      "2300 cost:[ 11.19805 ]  W: [[0.8274052 ]\n",
      " [0.15804973]\n",
      " [1.017992  ]]  b: [1.2241818]\n",
      "2400 cost:[ 11.100926 ]  W: [[0.823203  ]\n",
      " [0.16221294]\n",
      " [1.0180707 ]]  b: [1.2188805]\n",
      "2500 cost:[ 11.000489 ]  W: [[0.8188037]\n",
      " [0.1665573]\n",
      " [1.0181689]]  b: [1.2132845]\n",
      "2600 cost:[ 10.896802 ]  W: [[0.814203  ]\n",
      " [0.17108516]\n",
      " [1.0182868 ]]  b: [1.2073808]\n",
      "2700 cost:[ 10.789942 ]  W: [[0.80939615]\n",
      " [0.1757984 ]\n",
      " [1.0184275 ]]  b: [1.2011547]\n",
      "2800 cost:[ 10.679967 ]  W: [[0.80437857]\n",
      " [0.18069912]\n",
      " [1.0185937 ]]  b: [1.194592]\n",
      "2900 cost:[ 10.56698 ]  W: [[0.7991465 ]\n",
      " [0.18578896]\n",
      " [1.0187874 ]]  b: [1.1876774]\n",
      "3000 cost:[ 10.45109 ]  W: [[0.79369587]\n",
      " [0.19106917]\n",
      " [1.0190127 ]]  b: [1.180394]\n",
      "3100 cost:[ 10.332489 ]  W: [[0.78802305]\n",
      " [0.19654058]\n",
      " [1.0192716 ]]  b: [1.172725]\n",
      "3200 cost:[ 10.211248 ]  W: [[0.78212416]\n",
      " [0.20220359]\n",
      " [1.0195674 ]]  b: [1.1646522]\n",
      "3300 cost:[ 10.0875845 ]  W: [[0.7759963]\n",
      " [0.208058 ]\n",
      " [1.0199046]]  b: [1.1561564]\n",
      "3400 cost:[ 9.961639 ]  W: [[0.7696365 ]\n",
      " [0.21410288]\n",
      " [1.020286  ]]  b: [1.1472173]\n",
      "3500 cost:[ 9.833644 ]  W: [[0.76304245]\n",
      " [0.2203369 ]\n",
      " [1.0207157 ]]  b: [1.1378136]\n",
      "3600 cost:[ 9.703811 ]  W: [[0.75621235]\n",
      " [0.22675808]\n",
      " [1.0211979 ]]  b: [1.1279231]\n",
      "3700 cost:[ 9.572376 ]  W: [[0.749145  ]\n",
      " [0.23336348]\n",
      " [1.0217367 ]]  b: [1.1175226]\n",
      "3800 cost:[ 9.4395895 ]  W: [[0.7418398 ]\n",
      " [0.24014974]\n",
      " [1.0223365 ]]  b: [1.1065874]\n",
      "3900 cost:[ 9.305716 ]  W: [[0.73429704]\n",
      " [0.24711221]\n",
      " [1.0230016 ]]  b: [1.0950911]\n",
      "4000 cost:[ 9.171056 ]  W: [[0.72651756]\n",
      " [0.2542456 ]\n",
      " [1.0237368 ]]  b: [1.0830073]\n",
      "4100 cost:[ 9.035932 ]  W: [[0.7185034 ]\n",
      " [0.26154354]\n",
      " [1.0245469 ]]  b: [1.0703057]\n",
      "4200 cost:[ 8.900616 ]  W: [[0.7102574 ]\n",
      " [0.26899865]\n",
      " [1.0254364 ]]  b: [1.0569568]\n",
      "4300 cost:[ 8.765504 ]  W: [[0.70178354]\n",
      " [0.27660266]\n",
      " [1.02641   ]]  b: [1.0429292]\n",
      "4400 cost:[ 8.630913 ]  W: [[0.69308674]\n",
      " [0.2843461 ]\n",
      " [1.0274727 ]]  b: [1.0281893]\n",
      "4500 cost:[ 8.497148 ]  W: [[0.6841733]\n",
      " [0.2922184]\n",
      " [1.0286288]]  b: [1.0127026]\n",
      "4600 cost:[ 8.364637 ]  W: [[0.675051  ]\n",
      " [0.30020782]\n",
      " [1.029883  ]]  b: [0.996432]\n",
      "4700 cost:[ 8.233723 ]  W: [[0.6657285]\n",
      " [0.3083017]\n",
      " [1.0312397]]  b: [0.9793386]\n",
      "4800 cost:[ 8.104744 ]  W: [[0.6562165 ]\n",
      " [0.31648582]\n",
      " [1.0327028 ]]  b: [0.9613817]\n",
      "4900 cost:[ 7.9780827 ]  W: [[0.6465267]\n",
      " [0.3247453]\n",
      " [1.0342757]]  b: [0.94251955]\n",
      "5000 cost:[ 7.8540936 ]  W: [[0.6366726 ]\n",
      " [0.33306414]\n",
      " [1.0359619 ]]  b: [0.92270726]\n",
      "5100 cost:[ 7.7331133 ]  W: [[0.62666947]\n",
      " [0.34142506]\n",
      " [1.0377638 ]]  b: [0.9018986]\n",
      "5200 cost:[ 7.615467 ]  W: [[0.61653405]\n",
      " [0.34980994]\n",
      " [1.0396835 ]]  b: [0.8800453]\n",
      "5300 cost:[ 7.5014796 ]  W: [[0.6062846 ]\n",
      " [0.35819975]\n",
      " [1.0417223 ]]  b: [0.8570961]\n",
      "5400 cost:[ 7.391431 ]  W: [[0.5959413]\n",
      " [0.3665745]\n",
      " [1.0438807]]  b: [0.8329988]\n",
      "5500 cost:[ 7.2855864 ]  W: [[0.5855259 ]\n",
      " [0.37491405]\n",
      " [1.0461583 ]]  b: [0.807698]\n",
      "5600 cost:[ 7.1841674 ]  W: [[0.5750615 ]\n",
      " [0.38319704]\n",
      " [1.0485541 ]]  b: [0.7811359]\n",
      "5700 cost:[ 7.087413 ]  W: [[0.5645733 ]\n",
      " [0.39140218]\n",
      " [1.0510646 ]]  b: [0.7532542]\n",
      "5800 cost:[ 6.9954624 ]  W: [[0.55408746]\n",
      " [0.39950785]\n",
      " [1.0536869 ]]  b: [0.7239916]\n",
      "5900 cost:[ 6.9084277 ]  W: [[0.5436317 ]\n",
      " [0.40749225]\n",
      " [1.0564158 ]]  b: [0.69328433]\n",
      "6000 cost:[ 6.8264375 ]  W: [[0.5332344]\n",
      " [0.4153338]\n",
      " [1.0592456]]  b: [0.661068]\n",
      "6100 cost:[ 6.7495027 ]  W: [[0.5229257]\n",
      " [0.4230117]\n",
      " [1.0621685]]  b: [0.62727624]\n",
      "6200 cost:[ 6.6776304 ]  W: [[0.5127359 ]\n",
      " [0.43050486]\n",
      " [1.0651757 ]]  b: [0.59184086]\n",
      "6300 cost:[ 6.6107936 ]  W: [[0.50269586]\n",
      " [0.4377934 ]\n",
      " [1.068258  ]]  b: [0.55469304]\n",
      "6400 cost:[ 6.548876 ]  W: [[0.49283707]\n",
      " [0.444859  ]\n",
      " [1.0714039 ]]  b: [0.5157626]\n",
      "6500 cost:[ 6.4918065 ]  W: [[0.48319063]\n",
      " [0.45168415]\n",
      " [1.0746005 ]]  b: [0.47497973]\n",
      "6600 cost:[ 6.439375 ]  W: [[0.47378755]\n",
      " [0.45825276]\n",
      " [1.0778344 ]]  b: [0.4322743]\n",
      "6700 cost:[ 6.391403 ]  W: [[0.46465686]\n",
      " [0.46454993]\n",
      " [1.0810927 ]]  b: [0.38757586]\n",
      "6800 cost:[ 6.347644 ]  W: [[0.45582837]\n",
      " [0.4705638 ]\n",
      " [1.084359  ]]  b: [0.34081647]\n",
      "6900 cost:[ 6.307857 ]  W: [[0.44732824]\n",
      " [0.47628307]\n",
      " [1.0876199 ]]  b: [0.2919284]\n",
      "7000 cost:[ 6.2717524 ]  W: [[0.4391821 ]\n",
      " [0.48169985]\n",
      " [1.0908586 ]]  b: [0.24084675]\n",
      "7100 cost:[ 6.23905 ]  W: [[0.4314128]\n",
      " [0.4868078]\n",
      " [1.0940602]]  b: [0.1875099]\n",
      "7200 cost:[ 6.2094145 ]  W: [[0.42403954]\n",
      " [0.4916031 ]\n",
      " [1.0972102 ]]  b: [0.13185851]\n",
      "7300 cost:[ 6.1825533 ]  W: [[0.41707945]\n",
      " [0.49608427]\n",
      " [1.1002941 ]]  b: [0.07383863]\n",
      "7400 cost:[ 6.1581535 ]  W: [[0.41054633]\n",
      " [0.5002524 ]\n",
      " [1.1032981 ]]  b: [0.01340152]\n",
      "7500 cost:[ 6.1358943 ]  W: [[0.40444893]\n",
      " [0.50410974]\n",
      " [1.1062119 ]]  b: [-0.04949686]\n",
      "7600 cost:[ 6.1155205 ]  W: [[0.3987935]\n",
      " [0.5076625]\n",
      " [1.109024 ]]  b: [-0.11489242]\n",
      "7700 cost:[ 6.0967493 ]  W: [[0.39358172]\n",
      " [0.5109183 ]\n",
      " [1.1117253 ]]  b: [-0.18281424]\n",
      "7800 cost:[ 6.0792775 ]  W: [[0.38881117]\n",
      " [0.51388675]\n",
      " [1.1143094 ]]  b: [-0.2532832]\n",
      "7900 cost:[ 6.062937 ]  W: [[0.3844756]\n",
      " [0.5165795]\n",
      " [1.1167711]]  b: [-0.3263104]\n",
      "8000 cost:[ 6.047495 ]  W: [[0.38056427]\n",
      " [0.51900977]\n",
      " [1.1191077 ]]  b: [-0.4018982]\n",
      "8100 cost:[ 6.0327992 ]  W: [[0.37706324]\n",
      " [0.52119195]\n",
      " [1.1213189 ]]  b: [-0.48003763]\n",
      "8200 cost:[ 6.0186763 ]  W: [[0.3739551]\n",
      " [0.5231419]\n",
      " [1.1234053]]  b: [-0.5607089]\n",
      "8300 cost:[ 6.0050135 ]  W: [[0.3712187]\n",
      " [0.5248766]\n",
      " [1.1253715]]  b: [-0.6438801]\n",
      "8400 cost:[ 5.9917164 ]  W: [[0.36883104]\n",
      " [0.5264135 ]\n",
      " [1.1272213 ]]  b: [-0.7295076]\n",
      "8500 cost:[ 5.9787245 ]  W: [[0.36676633]\n",
      " [0.52776945]\n",
      " [1.1289629 ]]  b: [-0.8175354]\n",
      "8600 cost:[ 5.965979 ]  W: [[0.36499742]\n",
      " [0.5289623 ]\n",
      " [1.1306044 ]]  b: [-0.9078938]\n",
      "8700 cost:[ 5.953436 ]  W: [[0.3634968]\n",
      " [0.5300103]\n",
      " [1.1321536]]  b: [-1.0004991]\n",
      "8800 cost:[ 5.9410996 ]  W: [[0.3622361]\n",
      " [0.53093  ]\n",
      " [1.1336213]]  b: [-1.0952535]\n",
      "8900 cost:[ 5.9289503 ]  W: [[0.3611879]\n",
      " [0.531738 ]\n",
      " [1.1350161]]  b: [-1.1920458]\n",
      "9000 cost:[ 5.917019 ]  W: [[0.36032444]\n",
      " [0.5324503 ]\n",
      " [1.1363475 ]]  b: [-1.2907497]\n",
      "9100 cost:[ 5.905306 ]  W: [[0.35961986]\n",
      " [0.53308076]\n",
      " [1.1376263 ]]  b: [-1.3912233]\n",
      "9200 cost:[ 5.8938355 ]  W: [[0.35905024]\n",
      " [0.5336427 ]\n",
      " [1.1388601 ]]  b: [-1.4933105]\n",
      "9300 cost:[ 5.8826513 ]  W: [[0.3585927]\n",
      " [0.5341479]\n",
      " [1.1400579]]  b: [-1.596841]\n",
      "9400 cost:[ 5.8717527 ]  W: [[0.35822728]\n",
      " [0.5346071 ]\n",
      " [1.1412262 ]]  b: [-1.7016276]\n",
      "9500 cost:[ 5.8611884 ]  W: [[0.35793638]\n",
      " [0.5350294 ]\n",
      " [1.1423706 ]]  b: [-1.8074694]\n",
      "9600 cost:[ 5.8509874 ]  W: [[0.35770503]\n",
      " [0.5354228 ]\n",
      " [1.1434958 ]]  b: [-1.9141494]\n",
      "9700 cost:[ 5.8411846 ]  W: [[0.35751998]\n",
      " [0.5357934 ]\n",
      " [1.1446054 ]]  b: [-2.0214372]\n",
      "9800 cost:[ 5.8317833 ]  W: [[0.35737002]\n",
      " [0.5361461 ]\n",
      " [1.1457028 ]]  b: [-2.129089]\n",
      "9900 cost:[ 5.8228307 ]  W: [[0.35724655]\n",
      " [0.53648496]\n",
      " [1.1467893 ]]  b: [-2.2368453]\n",
      "10000 cost:[ 5.8143287 ]  W: [[0.35714272]\n",
      " [0.53681314]\n",
      " [1.1478652 ]]  b: [-2.344436]\n",
      "***** Learning Finished!!\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작\n",
    "print('***** Start Learning!!')\n",
    "for step in range(10001):\n",
    "    # cost를 minimize 한다\n",
    "    optimizer.minimize(cost_func,var_list=[W,b])\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        print('%04d'%step,'cost:[',cost_func().numpy(),']',\n",
    "             ' W:',W.numpy(),' b:',b.numpy())\n",
    "print('***** Learning Finished!!')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight: [[0.35714272]\n",
      " [0.53681314]\n",
      " [1.1478652 ]]\n",
      "Bias: [-2.344436]\n"
     ]
    }
   ],
   "source": [
    "# 회귀 계수, weight과 bias 출력\n",
    "print('Weight:',W.numpy())\n",
    "print('Bias:',b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Predict\n",
      "[[152.76192]\n",
      " [184.86086]\n",
      " [181.59914]\n",
      " [199.33546]\n",
      " [139.50722]]\n"
     ]
    }
   ],
   "source": [
    "# 예측\n",
    "print('***** Predict')\n",
    "x_data = [[73.,80.,75.],\n",
    "          [93.,88.,93.],\n",
    "          [89.,91.,90.],\n",
    "          [96.,98.,100.],\n",
    "          [73.,66.,70.]]\n",
    "x_test = np.array(x_data,dtype=np.float32)\n",
    "print(hypothesis(x_test).numpy())\n",
    "# 원본\n",
    "# 73,80,75,152\n",
    "# 93,88,93,185\n",
    "# 89,91,90,180\n",
    "# 96,98,100,196\n",
    "# 73,66,70,142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2.4112918\n"
     ]
    }
   ],
   "source": [
    "# 정확도 측정 : RMSE(Root Mean Squared Error)\n",
    "def get_rmse(y_test,preds):\n",
    "    squared_error = 0\n",
    "    for k,_ in enumerate(y_test):\n",
    "        squared_error += (preds[k] - y_test[k])**2\n",
    "    mse = squared_error/len(y_test)  \n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse[0]\n",
    "\n",
    "# 학습한 데이터를 그대로 검증 데이터로 사용한 경우\n",
    "x_test = x_train\n",
    "y_test = y_train\n",
    "\n",
    "preds = hypothesis(x_test).numpy()\n",
    "print('RMSE:',get_rmse(y_test,preds))  # RMSE: 2.4112918   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
