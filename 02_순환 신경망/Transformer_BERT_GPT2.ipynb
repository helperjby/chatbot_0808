{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer BERT GPT2 예제\n",
        "[Colab에서 실행]"
      ],
      "metadata": {
        "id": "4yqoRr2JVmY_"
      },
      "id": "4yqoRr2JVmY_"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "728ccdf2-f904-4c73-bae4-a7d0a4ea07a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "728ccdf2-f904-4c73-bae4-a7d0a4ea07a4",
        "outputId": "9c5fb8e3-8728-4d7c-b49f-f0eef0699940"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.9587\n",
            "Epoch [20/100], Loss: 0.5575\n",
            "Epoch [30/100], Loss: 0.3296\n",
            "Epoch [40/100], Loss: 0.1702\n",
            "Epoch [50/100], Loss: 0.0730\n",
            "Epoch [60/100], Loss: 0.0267\n",
            "Epoch [70/100], Loss: 0.0090\n",
            "Epoch [80/100], Loss: 0.0030\n",
            "Epoch [90/100], Loss: 0.0010\n",
            "Epoch [100/100], Loss: 0.0004\n",
            "Predicted: tensor([1])\n"
          ]
        }
      ],
      "source": [
        "# ! pip install torch torchvision\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 신경망 모델 정의\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# 모델, 손실 함수, 옵티마이저 정의\n",
        "model = SimpleNN(input_size=3, hidden_size=5, output_size=2)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# 예제 입력과 타겟 데이터\n",
        "inputs = torch.tensor([[1.0, 2.0, 3.0]])\n",
        "targets = torch.tensor([[0.0, 1.0]])\n",
        "\n",
        "# 모델 학습\n",
        "for epoch in range(100):\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')\n",
        "\n",
        "# 모델 평가\n",
        "with torch.no_grad():\n",
        "    outputs = model(inputs)\n",
        "    predicted = torch.argmax(outputs, dim=1)\n",
        "    print(f'Predicted: {predicted}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "POw6G6RGVj3f"
      },
      "id": "POw6G6RGVj3f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer\n",
        ": 구글이 2017년에 발표한 자연어 처리(NLP) 모델로, 그 이후로 많은 NLP 작업에서 획기적인 성능 향상을 이룬 모델이다. Transformer의 주요 혁신은 기존의 순환 신경망(RNN)이나 LSTM(Long Short-Term Memory)과 달리, 시퀀스를 처리할 때 순차적인 계산이 필요 없다는 점이다. 이로 인해 병렬 처리가 가능해졌고, 더 큰 모델을 효과적으로 학습할 수 있다"
      ],
      "metadata": {
        "id": "BU5XUGBnQRtp"
      },
      "id": "BU5XUGBnQRtp"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Transformer 모델 정의\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, model_dim, nhead, num_encoder_layers, num_decoder_layers, output_dim):\n",
        "        super(TransformerModel, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, model_dim)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=model_dim,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers\n",
        "        )\n",
        "        self.fc_out = nn.Linear(model_dim, output_dim)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src = self.embedding(src)  # (src_seq_len, N, model_dim)\n",
        "        tgt = self.embedding(tgt)  # (tgt_seq_len, N, model_dim)\n",
        "\n",
        "        # Transformer expects (seq_len, batch_size, model_dim)\n",
        "        output = self.transformer(src, tgt)\n",
        "        output = self.fc_out(output)\n",
        "        return output\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "input_dim = 10  # 예시 입력 차원 (어휘 크기)\n",
        "model_dim = 512  # 모델 차원\n",
        "nhead = 8  # 멀티헤드 어텐션의 헤드 수\n",
        "num_encoder_layers = 3  # 인코더 레이어 수\n",
        "num_decoder_layers = 3  # 디코더 레이어 수\n",
        "output_dim = 10  # 예시 출력 차원 (어휘 크기)\n",
        "\n",
        "# 모델 인스턴스 생성\n",
        "model = TransformerModel(input_dim, model_dim, nhead, num_encoder_layers, num_decoder_layers, output_dim)\n",
        "\n",
        "# 입력 및 출력 시퀀스 예시 (텐서로 변환)\n",
        "src = torch.randint(0, input_dim, (5, 1))  # (src_seq_len, batch_size)\n",
        "tgt = torch.randint(0, input_dim, (5, 1))  # (tgt_seq_len, batch_size)\n",
        "\n",
        "# 모델 출력 계산\n",
        "output = model(src, tgt)\n",
        "\n",
        "print(\"Model output shape:\", output.shape)\n",
        "print(\"Model output:\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOezeGe7PeM6",
        "outputId": "91584dcc-7390-4e75-9b37-6cdf7ac3019c"
      },
      "id": "NOezeGe7PeM6",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model output shape: torch.Size([5, 1, 10])\n",
            "Model output: tensor([[[ 0.1498,  1.3707, -0.6535,  0.3827,  0.2009,  0.2919, -0.5569,\n",
            "           0.1602, -0.0967, -0.0332]],\n",
            "\n",
            "        [[ 1.0357,  1.0251, -0.7247, -0.0464,  0.1237,  0.0134,  0.2956,\n",
            "          -0.2458,  0.6371, -0.4632]],\n",
            "\n",
            "        [[ 0.7356, -0.0029, -1.2712, -0.0243,  0.0728,  0.6858,  0.6377,\n",
            "          -0.0541,  0.0603, -0.4277]],\n",
            "\n",
            "        [[ 0.9750,  1.2079, -0.0882, -0.4421, -0.3538,  0.4816, -0.0931,\n",
            "          -0.1144,  0.0105, -0.3523]],\n",
            "\n",
            "        [[ 0.9583,  1.4565, -1.3288, -0.5301,  0.8749,  0.8493,  0.4207,\n",
            "          -0.2496,  0.0946, -0.5999]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT(Bidirectional Encoder Representations from Transformers)\n",
        ": Google에서 개발한 자연어 처리(NLP) 모델로, 다양한 NLP 작업에서 혁신적인 성능을 보여주는 모델이다. BERT는 주어진 텍스트의 문맥을 양방향으로 이해할 수 있도록 설계된 최초의 모델 중 하나이다."
      ],
      "metadata": {
        "id": "pTvSfa2EPmLh"
      },
      "id": "pTvSfa2EPmLh"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.nn.functional import softmax\n",
        "\n",
        "# 1. BERT 모델과 토크나이저 로드\n",
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# 2. 예제 문장 정의\n",
        "sentences = [\"I love this movie!\", \"This is the worst product I have ever bought.\"]\n",
        "\n",
        "# 3. 입력 텍스트를 BERT 모델 입력 형식으로 변환\n",
        "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# 4. 모델에 입력 데이터 전달하여 예측 수행\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# 5. 결과 해석\n",
        "logits = outputs.logits\n",
        "probs = softmax(logits, dim=1)\n",
        "predicted_class = torch.argmax(probs, dim=1)\n",
        "\n",
        "# 클래스 레이블 (BERT 모델에 따라 달라질 수 있음)\n",
        "class_labels = [\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"]\n",
        "\n",
        "# 6. 예측 결과 출력\n",
        "for sentence, pred in zip(sentences, predicted_class):\n",
        "    print(f\"Sentence: '{sentence}' is classified as '{class_labels[pred]}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GS87__4mPM5F",
        "outputId": "1d373642-89be-4078-e3e1-dc2aed50f001"
      },
      "id": "GS87__4mPM5F",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: 'I love this movie!' is classified as 'very positive'\n",
            "Sentence: 'This is the worst product I have ever bought.' is classified as 'very negative'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT-2(GPT-2, Generative Pre-trained Transformer 2)는\n",
        ": OpenAI에서 개발한 대규모 언어 모델로, 2019년에 발표.\n",
        "Transformer 아키텍처를 기반. Transformer는 병렬 처리가 가능하고, 긴 문맥을 잘 처리할 수 있어 NLP 작업에서 매우 효과적이다.이 모델은 자연어 생성, 번역, 요약 등 다양한 자연어 처리 작업에서 좋은 성능을 보여주었다."
      ],
      "metadata": {
        "id": "I82kPVEzU1Q4"
      },
      "id": "I82kPVEzU1Q4"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1b198a73-68c9-4ba9-bccf-70294dd3cae3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b198a73-68c9-4ba9-bccf-70294dd3cae3",
        "outputId": "1f787121-2bc1-4943-cd2b-daa00470d404"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------\n",
            "Once upon a time, it seems to be the most reliable way to start a conversation.\n",
            "\n",
            "It's also the one way to get an idea of how many people have been talking to you.\n",
            "\n",
            "It's also one way to get to\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# 1. GPT-2 모델과 토크나이저 로드\n",
        "model_name = \"gpt2\"  # 또는 \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\" 등\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# 2. 텍스트 프롬프트 정의\n",
        "prompt = \"Once upon a time\"\n",
        "\n",
        "# 3. 입력 텍스트를 GPT-2 모델 입력 형식으로 변환\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# 4. 텍스트 생성\n",
        "output_sequences = model.generate(\n",
        "    input_ids=inputs['input_ids'],\n",
        "    max_length=50,  # 생성할 텍스트의 최대 길이\n",
        "    temperature=0.7,  # 텍스트의 다양성 조절 (낮은 값: 더 예측 가능, 높은 값: 더 무작위)\n",
        "    top_k=50,  # 상위 k개 단어에서 샘플링\n",
        "    top_p=0.95,  # 상위 p% 확률 분포에서 샘플링 (nucleus sampling)\n",
        "    do_sample=True,  # 샘플링 사용 여부\n",
        ")\n",
        "\n",
        "# 5. 결과 해석 및 출력\n",
        "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
        "print('-----------------')\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# 1. GPT-2 모델과 토크나이저 로드\n",
        "model_name = \"gpt2\"  # 또는 \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# 2. 요약할 긴 텍스트 입력\n",
        "text_to_summarize = (\n",
        "    #\"GPT-2 is a large-scale unsupervised language model that generates coherent paragraphs of text. \"\n",
        "    \"It is the successor of GPT, developed by OpenAI. GPT-2 is capable of generating realistic and coherent text in a \"\n",
        "    \"variety of styles and for a range of applications, without needing task-specific training data.\"\n",
        ")\n",
        "\n",
        "\n",
        "# 3. 요약을 유도하는 프롬프트 정의\n",
        "prompt = \"Summarize the following text:\\n\\n\" + text_to_summarize + \"\\n\\nSummary:\"\n",
        "\n",
        "# 4. 입력 텍스트를 GPT-2 모델 입력 형식으로 변환\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# 5. 텍스트 생성 (요약)\n",
        "output_sequences = model.generate(\n",
        "    input_ids=inputs['input_ids'],\n",
        "    max_length=100,  # 생성할 텍스트의 최대 길이\n",
        "    temperature=0.2,  # 텍스트의 다양성 조절\n",
        "    top_k=50,  # 상위 k개 단어에서 샘플링\n",
        "    top_p=0.95,  # 상위 p% 확률 분포에서 샘플링\n",
        "    do_sample=True,  # 샘플링 사용 여부\n",
        "    num_return_sequences=1  # 반환할 요약 문장 수\n",
        ")\n",
        "\n",
        "# 6. 결과 해석 및 출력\n",
        "summary = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
        "print(\"Summary:\", summary)  # 성능이 좋지 않음, 영어위주로 학습되어 한글 지원 미비\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwfawys-OdLj",
        "outputId": "4057026d-aafa-4f0e-a96c-3065a91fdde1"
      },
      "id": "bwfawys-OdLj",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary: Summarize the following text:\n",
            "\n",
            "It is the successor of GPT, developed by OpenAI. GPT-2 is capable of generating realistic and coherent text in a variety of styles and for a range of applications, without needing task-specific training data.\n",
            "\n",
            "Summary:\n",
            "\n",
            "The GPT-2 is a new approach to the problem of text generation. It is based on the concept of a \"text-generating machine\", which is a computer program that generates text from\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XxNgKgH7WPSj"
      },
      "id": "XxNgKgH7WPSj",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}